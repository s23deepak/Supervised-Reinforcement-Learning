# Synthetic Data Kit Configuration for SRL QA (Seating + Blood Relation)
llm:
  provider: "vllm"

vllm:
  api_base: "http://localhost:8000/v1"
  model: "meta-llama/Llama-3.3-70B-Instruct"  # or your preferred model
  sleep_time: 0.1
  max_retries: 3

generation:
  temperature: 0.7
  top_p: 0.9
  num_pairs: 50          # pairs per chunk
  chunk_size: 4000       # character chunk size
  chunk_overlap: 200
  max_context_length: 8000

curate:
  threshold: 8.0         # quality score threshold (0-10)
  batch_size: 8

# CRITICAL: Custom prompts that emit numbered steps + Answer line
prompts:
  cot_generation: |
    You are a reasoning puzzle expert. Generate {num_pairs} high-quality reasoning puzzles with step-by-step solutions.

    For each puzzle, provide:
    1. "question": A clear, self-contained puzzle or query
    2. "cot": Step-by-step reasoning with NUMBERED STEPS, each formatted as "N. Title: content"
       - End with "Answer: <final_answer>"
    3. "answer": The final answer (short string)

    Topic context: {summary}

    Return ONLY valid JSON list:
    [
      {
        "question": "...",
        "cot": "1. Identify entities: ...
2. Analyze constraints: ...
3. Deduce answer: ...
Answer: ...",
        "answer": "..."
      },
      ...
    ]

  qa_generation: |
    Create {num_pairs} question-answer pairs grounded ONLY in the provided text.
    Return ONLY valid JSON: [{"question": "...", "answer": "..."}...]
